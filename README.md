# Knowledge-Distillation

![img](img/knowledge-distillation.png)

The term "Knowledge Distillation" (a.k.a Teacher-Student Model) was first introduced by (Bu-cilu et al., 2006; Ba & Caruana,2014) and has been popularized by (Hinton et al., 2015), as a way to let smaller deep learning models learn how bigger ones generalize to large datasets, hence increase the performance of the smaller one.

In this notebook I'll show you an intuitive way on how you implement Knowledge Distillation in Keras. 

